results <- 3+5-6
results
library(ggplot2)
ggplot(data = mpg,
aes(x = displ, y = hwy)) +
geom_point()
data(mpg, package = "ggplot2")
head(mpg)
summary(mpg)
help(mpg)
txt <- c(text1 = "This is $10 in 999 different ways,\n up and down; left and right!",
text2 = "@koheiw7 working: on #quanteda 2day\t4ever, http://textasdata.com?page=123.")
tokens(txt)
library("quanteda")
corp_uk <- corpus(data_char_ukimmig2010)
summary(corp_uk)
docvars(corp_uk, "Party") <- names(data_char_uimmig2010)
docvars(corp_uk, "Party") <- names(data_char_ukimmig2010)
docvars(copr_uk, "Year") <- 2010
docvars(corp_uk, "Year") <- 2010
summary(corp_uk)
require(readtext)
print(data_corpus_inaugural)
as.character(data_corpus_inaugural)[2]
as.character(data_corpus_inaugural)[3]
summary(data_corpus_inaugural, n = 5)
summary(data_corpus_inaugural, n = 10)
dat_json <- readtext("social_media/zombies/tweets.json")
tokeninfo <- summary(data_corpus_inaugural)
tokeninfo$Year <- docvars(data_corpus_inaugural, "Year")
with(tokeninfo, plot (Year, Tokens, type = "b", pch = 19, cex = .7))
tokeninfo[which.max(tokeninfo$Tokens)]
tokeninfo[which.max(tokeninfo$Tokens), ]
corp1 <- head(data_corpus_inaugural, 2)
corp2 <- tail(data_corpus_inaugural, 2)
corp3 <- corp1 + corp2
summary(corp3)
summary(corpus_subset(data_corpus_inaugural, Year > 1990))
summary(corpus_subset(data_corpus_inaugural, President == "Adams"))
data_tokens_inaugural <- tokens(data_corpus_inaugural)
kwic(data_tokens_inaugural, pattern = "terror")
kwic(data_tokens_inaugural, pattern = "terror", valuetype = "regex")
kwic(data_tokens_inaugural, pattern = "communist*")
kwic(data_tokens_inaugural, pattern = phrase ("United States")) |> head()
head(docvars(data_corpus_inaugural))
txt <- c(text1 = "This is $10 in 999 different ways,\n up and down; left and right!",
text2 = "@koheiw7 working: on #quanteda 2day\t4ever, http://textasdata.com?page=123.")
tokens(txt)
tokens(txt, remove_numbers = TRUE, remove_punct = TRUE)
tokens(txt, remove_numbers = FALSE, remove-punct = TRUE)
tokens(txt, remove_numbers = FALSE, remove_punct = TRUE)
tokens(txt, remove_numbers = TRUE,  remove_punct = FALSE)
tokens(txt, remove_numbers = FALSE, remove_punct = FALSE, remove_separators = FALSE)
install.packages("keyATM")
install.packages("keyATM")
install.packages("tidyverse")
install.packages("quanteda")
library(keyATM)
library(tidyverse)
library(quanteda)
data <- read_csv("success_survey - The Definition of Academic Success Survey_November 11, 2025_08.47.csv.numbers")
data <- read_csv("success_survey - The Definition of Academic Success Survey_November 11, 2025_08.47")
data <- read_csv("success_survey.csv")
data <- read_csv("/Users/miloscomputer/Downloads/success_survey.csv")
data <- read_csv ("success_survey - The Definition of Academic Success Survey_November 11, 2025_08.47.csv")
data <- read_csv("success_survey1.csv")
glimpse(data)
data <- data %>%
mutate()
data <- data %>%
mutate(definition_text = tolowere(definition_text), definition_text = str_replace_all(definition_text, "[^a-z\\s]", " "), definition_text = str_squish(definition_text) )
data <- data %>%
+ mutate(definition_text = tolower(definition_text), definition_text = str_replace_all(definition_text, "[^a-z\\s]", " "), definition_text = str_squish(definition_text) )
data <- data %>%
filter(!is.na(definition_text), definition_text != "")
names(data)
data <- data %>%
mutate(
definition = tolower(definition),
definition = str_replace_all(definition, "[^a-z\\s]", " "),
definition = str_squish(definition)
)
data <- data %>%
filter(!is.na(definition), definition != "")
data <- data %>%
mutate(
academicgoals = tolower(academicgoals),
academicgoals = str_replace_all(academicgoals, "[^a-z\\s]", " "),
academicgoals = str_squish(academicgoals)
)
filter(!is.na(academicgoals), academicgoals != "")
data <- data%>%
filter(!is.na(academicgoals), academicgoals != "")
data <- data%>%
mutate(
describeacademicgoals = tolower(describeacademicgoals),
describeacademicgoals = str_replace_all(describeacademicgoals, "[^a-z\\s]", " "),
describeacademicgoals = str_squish(describeacademicgoals)
)
data <- data %>%
filter(!is.na(describeacademicgoals), describeacademicgoals != "")
data <- data %>%
mutate(
resourceswhy = tolower(resourceswhy),
resourceswhy = str_replace_all(resourceswhy, "[^a-z\\s]", " "),
resourceswhy = str_squish(resourceswhy)
)
data <- data %>%
filter(!is.na(resourceswhy), resourceswhy != "")
data <- data%>%
mutate(
futureimpact = tolower(futureimpact),
futureimpact = str_replace_all(futureimpact, "[^a-z\\s]", " "),
futureimpact = str_squish(futureimpact)
)
data <- data %>%
filter(!is.na(futureimpact), futureimpact != "")
corpus <- corpus(data, text_field = "definition")
toks <- tokens(
corpus,
remove_punct = TRUE,
remove_numbers = TRUE
) %>%
tokens_remove(stopwords("en"))
dfm_success <- dfm(toks)
keywords <- list(
grades   = c("grade", "grades", "gpa", "test", "score", "scores", "sat", "exam"),
future   = c("college", "career", "job", "future", "opportunity", "admission"),
learning = c("learning", "curiosity", "understand", "mastery", "improve", "growth", "challenge"),
)
keywords <- list(
grades = c("grade", "grades", "exams", "exam", "test", "score"),
future = c("college", "career", "profession", "job", "future", "opportunity", "admission")
learning = c("learning", "curiosity", "understand", "mastery", "improve", "growtj", "challenge"),
keywords <- list(
grades = c("grade", "grades", "exams", "exam", "test", "score"),
future = c("college", "career", "profession", "job", "future", "opportunity", "admission"),
learning = c("learning", "curiosity", "understand", "mastery", "improve", "growth", "challenge"),
balance  = c("happy", "happiness", "mental", "balance", "stress", "wellbeing", "burnout")
)
keyatm_docs <- keyATM_read(dfm_success)
set.seed(123)
model <- keyATM(
docs = keyatm_docs$docs,
vocab = keyatm_docs$vocab,
K = 10,
model = "base",
keywords = keywords,
options = list(seed = 123,
iter = 2000,
burnin = 500)
)
set.seed(123)
model <- keyATM(
docs = keyatm_docs,
model = "base",
no_keyword_topics = 5,
keywords = keywords,
options = list(
seed = 123,
iterations = 2000,
verbose = TRUE,
store_theta = TRUE
)
)
dfm_success <- drm(toks)
vocab <- keyatm_docs$vocab
sapply(keywords,function(kw) intersect(kw, vocab))
keywords_clean <- lapply(keywords, function(kw) intersect (kw, vocab))
keywords_clean
keywords_clean <- keywords_clean[sapply(keywords_clean, length) > 0]
model <- keyATM(
docs = keyatm_docs,
model = "base",
no_keyword_topics = 5,
keywords = keywords_clean,
options = list(seed = 123, iter = 2000, burnin = 500)
)
library(factoextra)
# iris.labels
iris
#iris.labels
iris.labels = iris$Species
table(iris.labels)
iris_data <- iris[1:4]
iris_data
# Scale the data to keep distance metrics unweighted
iris_data_scale <- scale(iris_data)
# Scale the data to keep distance metrics unweighted
iris_data_scale <- scale(iris_data)
iris_data_scale
#Distance; built into factoextra
iris_data <- dist(iris_data_scale)
?dist
?dist
?fviz_nbcluster
?fviz_nbclust
??fviz_nbclust
# Calculate number of clusters needed
# Within Sum Squares
fviz_nbcluster(iris_data_scale, kmeans, method = "wss") +
labs(subtitle = "Elbow method")
# Calculate number of clusters needed
# Within Sum Squares
fviz_nbclust(iris_data_scale, kmeans, method = "wss") +
labs(subtitle = "Elbow method")
# Calculate number of clusters needed
# Within Sum Squares
fviz_nbclust(iris_data_scale, kmeans, method = "wss") +
labs(subtitle = "Elbow method")
# Calculate number of clusters needed
# Within Sum Squares
fviz_nbclust(iris_data_scale, kmeans, method = "wss") +
labs(subtitle = "Elbow method")
# Calculate number of clusters needed
# Within Sum Squares
fviz_nbclust(iris_data_scale, kmeans, method = "wss") +
labs(subtitle = "Elbow method")
# Calculate how many clusters you need
# Within Sum Squares
fviz_nbclust(iris_data_scale, kmeans, method = "wss") +
labs(subtitle = "Elbow method")
# Calculate how many clusters you need
# Within Sum Squares
fviz_nbclust(iris_data_scale, kmeans, method = "wss") +
labs(subtitle = "Elbow method")
install.packages("factoextra")
library(factoextra)
# Calculate how many clusters you need
# Within Sum Squares
fviz_nbclust(iris_data_scale, kmeans, method = "wss") +
labs(subtitle = "Elbow method")
?km.out
??km.out
?km
?kmeans
print(km.out)
install.packages(kmeans)
print(km.out)
print(km.out)
print(km.out)
# Kmeans
km.out <- kmeans(iris_data_scale, centers =3, nstart = 100)
print(km.out)
# Visualize the clustering algorithm results
km.clusters <- km.out$cluster
rownames(iris_data_scale) <- iris$Species
rownames(iris_data_scale) <- iris$Species
iris_data_scale
paste(iris$Species, 1:dim(iris)[1], sep = "_")
iris_data_scale
paste(iris$Species, 1:dim(iris)[1], sep = "_")
iris_data_scale
paste(iris$Species, 1:dim(iris)[1], sep = "_")
paste(iris$Species, 1:dim(iris)[1], sep = "_")
rownames(iris_data_scale) <- paste(iris$Species, 1:dim(iris)[1], sep = "_")
iris_data_scale
fviz_cluster(list(data=iris_data_scale, cluster = km.clusters))
table(km.clusters, iris$Species)
# Calculate how many clusters you need
# Within Sum Squares
fviz_nbclust(iris_data_scale, kmeans, method = "wss") +
labs(subtitle = "Elbow method")
# Calculate how many clusters you need
# Within Sum Squares
fviz_nbclust(iris_data_scale, kmeans, method = "wss") +
labs(subtitle = "Elbow method")
table(km.clusters, iris$Species)
table(km.clusters, iris$Species)
table(km.clusters, iris$Species)
?iris
import.package(quanteda)
package.import(quanteda)
package.import(quanteda)
library(readtext)
# Read text files
raw_docs <- readtext("/Users/miloscomputer/Downloads/MainQuestionForKeyATM.txt",
encoding = "UTF-8"
)
key_token <- tokens(key_corpus)
key_dfm <- dfm(key_token)
key_dfm <- dfm(key_token)
# Read text files
raw_docs <- readtext("/Users/miloscomputer/Downloads/MainQuestionForKeyATM.txt",
encoding = "UTF-8"
)
# Preprocessing with quanteda and create a dfm object
key_corpus <- corpus(raw_docs, text_field = "text")
data_tokens <- tokens(key_corpus,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_separators = TRUE,
remove_url = TRUE
) %>%
tokens_tolower() %>%
tokens_remove(
c(
stopwords("english"),
"may", "shall", "can",
"must", "upon", "with", "without"
)
) %>%
tokens_select(min_nchar = 3)
data_tokens <- tokens(key_corpus,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_separators = TRUE,
remove_url = TRUE
)%>%
tokens_tolower()%>%
tokens_remove(
c(
stopwords("english"),
"may", "shall", "can",
"must", "upon", "with", "without"
)
)%>%
tokens_select(min_nchar = 3)
install.packages("magrittr")
data_tokens <- tokens(key_corpus,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_separators = TRUE,
remove_url = TRUE
) %>%
tokens_tolower() %>%
tokens_remove(
c(
stopwords("english"),
"may", "shall", "can",
"must", "upon", "with", "without"
)
) %>%
tokens_select(min_nchar = 3)
install.packages("magrittr")
install.packages("magrittr")
'%>%'
'%>%'
data_tokens <- tokens(key_corpus,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_separators = TRUE,
remove_url = TRUE
) |>
tokens_tolower() |>
tokens_remove(
c(
stopwords("english"),
"may", "shall", "can",
"must", "upon", "with", "without"
)
) |>
tokens_select(min_nchar = 3)
install.packages("magrittr")
install.packages("magrittr")
data_tokens <- tokens(key_corpus,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_separators = TRUE,
remove_url = TRUE
) |>
tokens_tolower() |>
tokens_remove(
c(
stopwords("english"),
"may", "shall", "can",
"must", "upon", "with", "without"
)
) |>
tokens_select(min_nchar = 3)
packageVersion("quanteda")
install.packages("quanteda")
library(quanteda)
data_tokens <- tokens(key_corpus,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_separators = TRUE,
remove_url = TRUE
) |>
tokens_tolower() |>
tokens_remove(
c(
stopwords("english"),
"may", "shall", "can",
"must", "upon", "with", "without"
)
) |>
tokens_select(min_nchar = 3)
install.packages("readtext")
data_tokens <- tokens(key_corpus,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_separators = TRUE,
remove_url = TRUE
) |>
tokens_tolower() |>
tokens_remove(
c(
stopwords("english"),
"may", "shall", "can",
"must", "upon", "with", "without"
)
) |>
tokens_select(min_nchar = 3)
source("~/Desktop/VSCodeForJava/DSJava/Learn_RStudio/keyATM.R")
key_corpus
ncol(data_dfm) # number of unique words
source("~/Desktop/VSCodeForJava/DSJava/Learn_RStudio/keyATM.R")
summary(keyATM_docs)
source("~/Desktop/VSCodeForJava/DSJava/Learn_RStudio/keyATM.R")
install.packages("keyATM")
install.packages("keyATM")
source("~/Desktop/VSCodeForJava/DSJava/Learn_RStudio/keyATM.R")
source("~/Desktop/VSCodeForJava/DSJava/Learn_RStudio/keyATM.R")
source("~/Desktop/VSCodeForJava/DSJava/Learn_RStudio/keyATM.R")
source("~/Desktop/VSCodeForJava/DSJava/Learn_RStudio/keyATM.R")
source("~/Desktop/VSCodeForJava/DSJava/Learn_RStudio/keyATM.R")
keywords <- list(
Parents        = c("parents", "mother", "father"),
Career         = c("major", "degree", "job"),
Grades         = c("classes", "grades", "studies")
)
key_viz
key_viz
key_viz <- visualize_keywords(docs = keyATM_docs, keywords = keywords)
key_viz <- visualize_keywords(docs = keyATM_docs, keywords = keywords)
source("~/Desktop/VSCodeForJava/DSJava/Learn_RStudio/keyATM.R")
key_viz <- visualize_keywords(docs = keyATM_docs, keywords = keywords)
key_viz
docs_withSplit <- keyATM_read(
texts = data_dfm,
split = 0.3
)
top_words(out)
source("~/Desktop/VSCodeForJava/DSJava/Learn_RStudio/keyATM.R")
top_words(out)
docs_withSplit <- keyATM_read(
texts = data_dfm,
split = 0.3
)
top_words(out)
out <- weightedLDA(
docs              = docs_with$W_split, # 30% of the corpus, meaning file
number_of_topics  = 10, # the number of potential themes in corpus
model             = "base",
options           = list(seed = 250)
)
source("~/Desktop/VSCodeForJava/DSJava/Learn_RStudio/keyATM.R")
rlang::last_trace()
source("~/Desktop/VSCodeForJava/DSJava/Learn_RStudio/keyATM.R")
saveRDS(out, file = "SAVENAME.rds")
out <- weightedLDA(
docs              = docs_withSplit$W_split, # 30% of the corpus, meaning file
number_of_topics  = 10, # the number of potential themes in corpus
model             = "base",
options           = list(seed = 250)
)
out <- readRDS(file = "SAVENAME.rds")
top_words(out)
source("~/Desktop/VSCodeForJava/DSJava/Learn_RStudio/keyATM.R")
top_words(out)
source("~/Desktop/VSCodeForJava/DSJava/Learn_RStudio/keyATM.R")
top_words(out)
source("~/Desktop/VSCodeForJava/DSJava/Learn_RStudio/keyATM.R")
top_words(out)
source("~/Desktop/VSCodeForJava/DSJava/Learn_RStudio/keyATM.R")
top_words(out)
top_words(out)
source("~/Desktop/VSCodeForJava/DSJava/Learn_RStudio/keyATM.R")
top_words(out)
source("~/Desktop/VSCodeForJava/DSJava/Learn_RStudio/keyATM.R")
top_words(out)
source("~/Desktop/VSCodeForJava/DSJava/Learn_RStudio/keyATM.R")
top_words(out)
plot_topicprop(out, show_topic = 1:5)
plot_topicprop(out, show_topic = 1:3)
plot_topicprop(out, show_topic = 1:1)
fig_modelfit
source("~/Desktop/VSCodeForJava/DSJava/Learn_RStudio/keyATM.R")
fig_modelfit
plot_pi(out)
source("~/Desktop/VSCodeForJava/DSJava/Learn_RStudio/keyATM.R")
plot_pi(out)
top_words(out)
fig_modelfit
source("~/Desktop/VSCodeForJava/DSJava/Learn_RStudio/keyATM.R")
fig_modelfit
